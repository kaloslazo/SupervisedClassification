{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50fe15c1-f5a1-422f-ad6c-cd7c4867d616",
   "metadata": {},
   "source": [
    "# **Proyecto 02: Clasificación Supervisada**\n",
    "\n",
    "---\n",
    "\n",
    "### **Integrantes**:\n",
    "- Kalos Lazo\n",
    "- Benjamín Soto\n",
    "- Lucas Carranza\n",
    "- José Osnayo\n",
    "\n",
    "---\n",
    "\n",
    "### **Importar librerías escenciales**\n",
    "\n",
    "A continuación se importarán las librerías escenciales para proceder con nuestro análisis exploratorio de datos (EDA), los archivos utilizan el formato `.h5` destinado para trabajar con archivos complejos, se procederá a utilizar la librería `h5py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08854a42-5d10-4b31-8f32-f3d4c1b71425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import make_forecasting_frame\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51103abe-9e9f-4492-8024-ec0953c60216",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Data**\n",
    "\n",
    "Considerando que dentro de nuestra estructura contamos con una carpeta de datos en la localización raíz `/data`, importamos los archivos train y test para entrenamiento y testeo respectivamente.\n",
    "\n",
    "Se procede a crear una [función](https://stackoverflow.com/questions/28170623/how-to-read-hdf5-files-in-python) para convertir de nuestros archivos de `.h5` a un pandas dataframe, de tal forma que podamos utilizar nuestros datos al aplicar el modelo. La función toma la ubicación del archivo y convierte cada dataset en un elemento del tipo numpy array, posteriormente se instancia como un objeto pandas dataframe.\n",
    "\n",
    "Por último una vez tenemos un diccionario de dataframes, se hace esto para mantener el nombre de cada dataset para un posterior análisis y correctitud. Esto se logra al ver que al sacar las features y juntar todas las características necesitamos un mecanismo que diferencie entre las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd57c511-21c8-4a88-acfa-e97cba494ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claves de columnas en train_data\n",
      ": dict_keys(['body_acc_x', 'body_acc_y', 'body_acc_z', 'body_gyro_x', 'body_gyro_y', 'body_gyro_z', 'total_acc_x', 'total_acc_y', 'total_acc_z', 'y'])\n",
      "\n",
      "Claves de columnas en test_data\n",
      ": dict_keys(['body_acc_x', 'body_acc_y', 'body_acc_z', 'body_gyro_x', 'body_gyro_y', 'body_gyro_z', 'total_acc_x', 'total_acc_y', 'total_acc_z'])\n"
     ]
    }
   ],
   "source": [
    "def file_data_processing(path):\n",
    "    with h5py.File(path, 'r') as hf:\n",
    "        return {each_dataset: pd.DataFrame(hf[each_dataset][()]) for each_dataset in hf}\n",
    "\n",
    "path = './data/'\n",
    "train_path = f'{path}/train.h5'\n",
    "test_path = f'{path}/test.h5'\n",
    "\n",
    "train_data = file_data_processing(train_path)\n",
    "test_data = file_data_processing(test_path)\n",
    "\n",
    "print(\"Claves de columnas en train_data\\n:\", train_data.keys())\n",
    "print(\"\\nClaves de columnas en test_data\\n:\", test_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931d4ab0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Extracción de características**\n",
    "\n",
    "Hasta ahora contamos con una matriz de matrices, donde en cada uno de sus hijos interiores representa un dataset del usuario para una respectiva acción con sus EEG, por ejemplo el de caminar hacia arriba, caminar hacia abajo, entre otras.\n",
    "\n",
    "Al tener 128 características por lo que debemos inicialmente reducir la dimensionalidad, utilizaremos la reducción por LDA (Linear Discriminant Analysis), una vez logramos esto individualmente, procedemos a juntarlos todos en una matriz. Si no hicieramos esta reducción trabajaríamos con una matriz de $128 \\cdot 9 = 1152$ características. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d3c1227",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en body_acc_x: (7352, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en body_acc_y: (7352, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en body_acc_z: (7352, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en body_gyro_x: (7352, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en body_gyro_y: (7352, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en body_gyro_z: (7352, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en total_acc_x: (7352, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en total_acc_y: (7352, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en total_acc_z: (7352, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 27.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en body_acc_x: (2947, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 34.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en body_acc_y: (2947, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 28.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en body_acc_z: (2947, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 29.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en body_gyro_x: (2947, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 27.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en body_gyro_y: (2947, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 26.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en body_gyro_z: (2947, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 35.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en total_acc_x: (2947, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 25.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en total_acc_y: (2947, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 26.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de características extraídas en total_acc_z: (2947, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def dataset_descriptive_features(df, dataset_name):\n",
    "    df_conversion_long = df.copy();\n",
    "    df_conversion_long['id'] = df.index;\n",
    "    df_conversion_long = df_conversion_long.melt(id_vars=['id'], var_name='time', value_name='value')\n",
    "    \n",
    "    extracted_features = extract_features(df_conversion_long, column_id = 'id', column_sort = 'time',  default_fc_parameters = MinimalFCParameters(), n_jobs=4)\n",
    "    extracted_features.columns = [f\"{dataset_name}_{each_col}\" for each_col in extracted_features.columns]\n",
    "    \n",
    "    return extracted_features\n",
    "\n",
    "def get_datasets_features(train_data):\n",
    "    features_list = [];\n",
    "    for name, df in train_data.items():\n",
    "        if name != 'y':\n",
    "            extracted_features = dataset_descriptive_features(df, name)\n",
    "            print(f\"Dimension de características extraídas en {name}: {extracted_features.shape}\")\n",
    "            features_list.append(extracted_features)\n",
    "    return features_list;\n",
    "\n",
    "x_train_features = get_datasets_features(train_data);\n",
    "x_test_features = get_datasets_features(test_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce365de1",
   "metadata": {},
   "source": [
    "Con la función anterior, `descriptive_features`, se logra obtener las características más importantes y descriptivas de cada dataset. Podríamos hacerlo manual obteniendo mediana, mínimo, máximo, entre otros, pero para este caso ya utilizamos funcionalidad hecha por la librería de [TsFresh](https://tsfresh.readthedocs.io/en/latest/): extrae automáticamente [características importantes](https://www.linkedin.com/pulse/unlocking-time-series-insights-tsfresh-python-guide-rany-5yr7c/) acerca de una serie temporal.\n",
    "\n",
    "Para aplicar este método primero debe convertirse el dataframe a un formato largo (long) que es exigido por la librería, se basa en una matriz tal que cada fila es una observación y cada columna un momento del tiempo, posteriormente ese train data sera dividido para el eje X, Y. El resultado esperado en `x_train_features` es almacenar una lista con las features más importantes obtenidas de cada dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8c5b1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (7352, 90)\n",
      "Dimensiones de Y_train: (7352, 1)\n",
      "Dimensiones de X_test: (2947, 90)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.concat(x_train_features, axis = 1).dropna(axis = 1).reset_index(drop = True)\n",
    "Y_train = pd.DataFrame(train_data['y'].to_numpy().flatten())\n",
    "\n",
    "X_test = pd.concat(x_test_features, axis=1).dropna(axis=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dimensiones de X_train: {X_train.shape}\")\n",
    "print(f\"Dimensiones de Y_train: {Y_train.shape}\")\n",
    "print(f\"Dimensiones de X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b04d13-22ab-4307-bacb-d658376653f0",
   "metadata": {},
   "source": [
    "Una vez realizado este proceso contaremos con nuevas columnas en nuestro `X_train`, pues hemos aplicado una funcionalidad de la librería TsFresh y hemos escarbado la información más importante de nuestra serie de tiempo (*time series*). Cabe resaltar que `X_train` está concatenando múltiples Dataframes en uno solo: `x_train_features`, la opción `axis = 1` indica que será por medio de las columnas, se eliminan las columnas con valores `NaN` y se elimina el indice anterior. En el caso de Y sólo se convierte en vector unidimensional con `flatten`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9480b6a-2b96-463e-b9c5-8b5fb21973f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['body_acc_x_value__sum_values', 'body_acc_x_value__median',\n",
       "       'body_acc_x_value__mean', 'body_acc_x_value__length',\n",
       "       'body_acc_x_value__standard_deviation', 'body_acc_x_value__variance',\n",
       "       'body_acc_x_value__root_mean_square', 'body_acc_x_value__maximum',\n",
       "       'body_acc_x_value__absolute_maximum', 'body_acc_x_value__minimum',\n",
       "       'body_acc_y_value__sum_values', 'body_acc_y_value__median',\n",
       "       'body_acc_y_value__mean', 'body_acc_y_value__length',\n",
       "       'body_acc_y_value__standard_deviation', 'body_acc_y_value__variance',\n",
       "       'body_acc_y_value__root_mean_square', 'body_acc_y_value__maximum',\n",
       "       'body_acc_y_value__absolute_maximum', 'body_acc_y_value__minimum',\n",
       "       'body_acc_z_value__sum_values', 'body_acc_z_value__median',\n",
       "       'body_acc_z_value__mean', 'body_acc_z_value__length',\n",
       "       'body_acc_z_value__standard_deviation', 'body_acc_z_value__variance',\n",
       "       'body_acc_z_value__root_mean_square', 'body_acc_z_value__maximum',\n",
       "       'body_acc_z_value__absolute_maximum', 'body_acc_z_value__minimum',\n",
       "       'body_gyro_x_value__sum_values', 'body_gyro_x_value__median',\n",
       "       'body_gyro_x_value__mean', 'body_gyro_x_value__length',\n",
       "       'body_gyro_x_value__standard_deviation', 'body_gyro_x_value__variance',\n",
       "       'body_gyro_x_value__root_mean_square', 'body_gyro_x_value__maximum',\n",
       "       'body_gyro_x_value__absolute_maximum', 'body_gyro_x_value__minimum',\n",
       "       'body_gyro_y_value__sum_values', 'body_gyro_y_value__median',\n",
       "       'body_gyro_y_value__mean', 'body_gyro_y_value__length',\n",
       "       'body_gyro_y_value__standard_deviation', 'body_gyro_y_value__variance',\n",
       "       'body_gyro_y_value__root_mean_square', 'body_gyro_y_value__maximum',\n",
       "       'body_gyro_y_value__absolute_maximum', 'body_gyro_y_value__minimum',\n",
       "       'body_gyro_z_value__sum_values', 'body_gyro_z_value__median',\n",
       "       'body_gyro_z_value__mean', 'body_gyro_z_value__length',\n",
       "       'body_gyro_z_value__standard_deviation', 'body_gyro_z_value__variance',\n",
       "       'body_gyro_z_value__root_mean_square', 'body_gyro_z_value__maximum',\n",
       "       'body_gyro_z_value__absolute_maximum', 'body_gyro_z_value__minimum',\n",
       "       'total_acc_x_value__sum_values', 'total_acc_x_value__median',\n",
       "       'total_acc_x_value__mean', 'total_acc_x_value__length',\n",
       "       'total_acc_x_value__standard_deviation', 'total_acc_x_value__variance',\n",
       "       'total_acc_x_value__root_mean_square', 'total_acc_x_value__maximum',\n",
       "       'total_acc_x_value__absolute_maximum', 'total_acc_x_value__minimum',\n",
       "       'total_acc_y_value__sum_values', 'total_acc_y_value__median',\n",
       "       'total_acc_y_value__mean', 'total_acc_y_value__length',\n",
       "       'total_acc_y_value__standard_deviation', 'total_acc_y_value__variance',\n",
       "       'total_acc_y_value__root_mean_square', 'total_acc_y_value__maximum',\n",
       "       'total_acc_y_value__absolute_maximum', 'total_acc_y_value__minimum',\n",
       "       'total_acc_z_value__sum_values', 'total_acc_z_value__median',\n",
       "       'total_acc_z_value__mean', 'total_acc_z_value__length',\n",
       "       'total_acc_z_value__standard_deviation', 'total_acc_z_value__variance',\n",
       "       'total_acc_z_value__root_mean_square', 'total_acc_z_value__maximum',\n",
       "       'total_acc_z_value__absolute_maximum', 'total_acc_z_value__minimum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a4af39-fe4f-469f-bc93-df68be944c73",
   "metadata": {},
   "source": [
    "En este caso podemos ver cómo reducimos la complejidad ahora con $90$ columnas de las cuales tenemos la información escencial de las líneas de tiempo, que anteriormente eran $128 \\cdot 9$. Con esta información nuestro modelo ya puede empezar a ser entrenado. Se verifica posteriormente que se preservan las filas iniciales, en este caso $7352$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac3719a6-77f7-4750-b653-f9a48cc18be5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_acc_x_value__sum_values</th>\n",
       "      <th>body_acc_x_value__median</th>\n",
       "      <th>body_acc_x_value__mean</th>\n",
       "      <th>body_acc_x_value__length</th>\n",
       "      <th>body_acc_x_value__standard_deviation</th>\n",
       "      <th>body_acc_x_value__variance</th>\n",
       "      <th>body_acc_x_value__root_mean_square</th>\n",
       "      <th>body_acc_x_value__maximum</th>\n",
       "      <th>body_acc_x_value__absolute_maximum</th>\n",
       "      <th>body_acc_x_value__minimum</th>\n",
       "      <th>...</th>\n",
       "      <th>total_acc_z_value__sum_values</th>\n",
       "      <th>total_acc_z_value__median</th>\n",
       "      <th>total_acc_z_value__mean</th>\n",
       "      <th>total_acc_z_value__length</th>\n",
       "      <th>total_acc_z_value__standard_deviation</th>\n",
       "      <th>total_acc_z_value__variance</th>\n",
       "      <th>total_acc_z_value__root_mean_square</th>\n",
       "      <th>total_acc_z_value__maximum</th>\n",
       "      <th>total_acc_z_value__absolute_maximum</th>\n",
       "      <th>total_acc_z_value__minimum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.290392</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>...</td>\n",
       "      <td>12.765670</td>\n",
       "      <td>0.099841</td>\n",
       "      <td>0.099732</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.003970</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.099811</td>\n",
       "      <td>0.109485</td>\n",
       "      <td>0.109485</td>\n",
       "      <td>0.088742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022239</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.006706</td>\n",
       "      <td>-0.006706</td>\n",
       "      <td>...</td>\n",
       "      <td>12.408253</td>\n",
       "      <td>0.097748</td>\n",
       "      <td>0.096939</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.097064</td>\n",
       "      <td>0.105788</td>\n",
       "      <td>0.105788</td>\n",
       "      <td>0.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.054796</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>-0.010483</td>\n",
       "      <td>...</td>\n",
       "      <td>11.890790</td>\n",
       "      <td>0.093636</td>\n",
       "      <td>0.092897</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.093100</td>\n",
       "      <td>0.105788</td>\n",
       "      <td>0.105788</td>\n",
       "      <td>0.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042157</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>-0.010483</td>\n",
       "      <td>...</td>\n",
       "      <td>11.219701</td>\n",
       "      <td>0.087501</td>\n",
       "      <td>0.087654</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.087793</td>\n",
       "      <td>0.098737</td>\n",
       "      <td>0.098737</td>\n",
       "      <td>0.076888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.024980</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.006847</td>\n",
       "      <td>-0.006847</td>\n",
       "      <td>...</td>\n",
       "      <td>10.879856</td>\n",
       "      <td>0.084765</td>\n",
       "      <td>0.084999</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.085077</td>\n",
       "      <td>0.093388</td>\n",
       "      <td>0.093388</td>\n",
       "      <td>0.074595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_acc_x_value__sum_values  body_acc_x_value__median  \\\n",
       "0                      0.290392                  0.002025   \n",
       "1                      0.022239                  0.000110   \n",
       "2                      0.054796                  0.000627   \n",
       "3                      0.042157                  0.000269   \n",
       "4                     -0.024980                 -0.000144   \n",
       "\n",
       "   body_acc_x_value__mean  body_acc_x_value__length  \\\n",
       "0                0.002269                     128.0   \n",
       "1                0.000174                     128.0   \n",
       "2                0.000428                     128.0   \n",
       "3                0.000329                     128.0   \n",
       "4               -0.000195                     128.0   \n",
       "\n",
       "   body_acc_x_value__standard_deviation  body_acc_x_value__variance  \\\n",
       "0                              0.002941                    0.000009   \n",
       "1                              0.001981                    0.000004   \n",
       "2                              0.002908                    0.000008   \n",
       "3                              0.002678                    0.000007   \n",
       "4                              0.002015                    0.000004   \n",
       "\n",
       "   body_acc_x_value__root_mean_square  body_acc_x_value__maximum  \\\n",
       "0                            0.003714                   0.010810   \n",
       "1                            0.001989                   0.005251   \n",
       "2                            0.002940                   0.008167   \n",
       "3                            0.002698                   0.008167   \n",
       "4                            0.002025                   0.005650   \n",
       "\n",
       "   body_acc_x_value__absolute_maximum  body_acc_x_value__minimum  ...  \\\n",
       "0                            0.010810                  -0.004294  ...   \n",
       "1                            0.006706                  -0.006706  ...   \n",
       "2                            0.010483                  -0.010483  ...   \n",
       "3                            0.010483                  -0.010483  ...   \n",
       "4                            0.006847                  -0.006847  ...   \n",
       "\n",
       "   total_acc_z_value__sum_values  total_acc_z_value__median  \\\n",
       "0                      12.765670                   0.099841   \n",
       "1                      12.408253                   0.097748   \n",
       "2                      11.890790                   0.093636   \n",
       "3                      11.219701                   0.087501   \n",
       "4                      10.879856                   0.084765   \n",
       "\n",
       "   total_acc_z_value__mean  total_acc_z_value__length  \\\n",
       "0                 0.099732                      128.0   \n",
       "1                 0.096939                      128.0   \n",
       "2                 0.092897                      128.0   \n",
       "3                 0.087654                      128.0   \n",
       "4                 0.084999                      128.0   \n",
       "\n",
       "   total_acc_z_value__standard_deviation  total_acc_z_value__variance  \\\n",
       "0                               0.003970                     0.000016   \n",
       "1                               0.004918                     0.000024   \n",
       "2                               0.006145                     0.000038   \n",
       "3                               0.004945                     0.000024   \n",
       "4                               0.003637                     0.000013   \n",
       "\n",
       "   total_acc_z_value__root_mean_square  total_acc_z_value__maximum  \\\n",
       "0                             0.099811                    0.109485   \n",
       "1                             0.097064                    0.105788   \n",
       "2                             0.093100                    0.105788   \n",
       "3                             0.087793                    0.098737   \n",
       "4                             0.085077                    0.093388   \n",
       "\n",
       "   total_acc_z_value__absolute_maximum  total_acc_z_value__minimum  \n",
       "0                             0.109485                    0.088742  \n",
       "1                             0.105788                    0.081100  \n",
       "2                             0.105788                    0.081100  \n",
       "3                             0.098737                    0.076888  \n",
       "4                             0.093388                    0.074595  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ab3e5-d60e-4b67-9575-248aff5c5eb7",
   "metadata": {},
   "source": [
    "---\n",
    "### **Exploración de Datos**\n",
    "\n",
    "Lo primero que queremos es conocer nuestras features que previamente han sido modificados, es así que como primer paso podemos usar la función `describe()` que permite de manera sencilla obtener estadísticas descriptivas de nuestras columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32a807-f508-4b76-bb28-d06cdd2acf7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b274c-5251-43ec-b8e6-d0741ec2bf1e",
   "metadata": {},
   "source": [
    "Antes de mostrar gráficamente estos datos, vamos a utilizar la información de `Y` que está numérica desde el `1 \\to 6` que indica la acción, por ejemplo \"WALKING\", por ello vamos a utilizar estos labels considerando el orden lógico propuesto en el enunciado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61903be6-5c44-41ac-bdbb-fbdeb3facbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_train[0].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210fedf1-aa76-4699-a0e8-56e54184201d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hemos creado una función para asignar cada elemento de `Y_train` a el obtenido dentro de sus filas, de esta forma contamos las apariciones y es posible gráficamente comparar y notar que `LAYING: 5`, es el que más apareció dentro de los registros de entrenamiento. Otra cosa a mencionar es que las actividades con su etiqueta asociada están distribuidas de manera equitativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf5c04-9e5d-4090-a5c6-0368ffc12d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_key_labels(df, arr_to_mapped):\n",
    "    activities_map = {i+1: arr_to_mapped[i] for i in range(len(arr_to_mapped))}\n",
    "    activities_mapped = df[0].map(activities_map);\n",
    "    df_mapped = pd.DataFrame({'Activity': activities_mapped});\n",
    "    \n",
    "    return df_mapped;\n",
    "    \n",
    "labels_actions = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING']\n",
    "Y_train_label = assign_key_labels(Y_train, labels_actions)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.countplot(x='Activity', data=Y_train_label, order=labels_actions)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title('Gráfico comparativo de cuenta de actividades en X_train')\n",
    "plt.xlabel('Nombre de Actividad')\n",
    "plt.ylabel('Conteo de aparición')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e2a9e-3483-4568-98aa-97ed48c025c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Procesamiento de datos**\n",
    "\n",
    "Ahora, se procede con la división de los conjuntos de datos, usando la función `train_test_split()` de la librería `sklearn.model_selection`, obtenemos los valores de entrenamiento y validación.\n",
    "\n",
    "No obstante se proceden a escalar los datos, tanto de entranemiento como de validación, esto es necesario para el modelo seleccionado de K-NN, se logra usando `StandardScaler()` de la librería `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d5655-e549-4cdb-bd86-74a44e420481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_split, X_val, Y_train_split, Y_val = train_test_split(X_train, Y_train, test_size = 0.3, random_state = 737)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_split = scaler.fit_transform(X_train_split)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa743e4-b7d1-422a-bdc9-f6c823ce8178",
   "metadata": {},
   "source": [
    "## Modelos de Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14969e91-9c6b-4dcf-af47-fcd5e60996d2",
   "metadata": {},
   "source": [
    "### **Modelo 1: K-NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca785654-5e0f-4e79-bdbb-73548d1d29df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return np.linalg.norm(p1 - p2, 2)\n",
    "\n",
    "\n",
    "class KdNode:\n",
    "\n",
    "    def __init__(self, point, label, idx_column):\n",
    "        self.point = point\n",
    "        self.idx_column = idx_column\n",
    "        self.label = label\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def show(self):\n",
    "        print(self.point)\n",
    "        print(self.label)\n",
    "\n",
    "\n",
    "\n",
    "class KdTree:\n",
    "    def __init__(self, data_points):\n",
    "        self.data_points = data_points\n",
    "        self.root = self._build_tree(data_points=data_points, depth=0)\n",
    "\n",
    "    def _build_tree(self, data_points, depth):\n",
    "        \"\"\"\n",
    "            Construct a KdTree with a set of data points.\n",
    "        :param data_points: a dataframe containing data points.\n",
    "        :param depth: actual depth of the tree.\n",
    "        :return: A KdNode object.\n",
    "        \"\"\"\n",
    "        if len(data_points) == 0:\n",
    "            return None\n",
    "\n",
    "        idx_column = depth % (data_points.shape[1] - 1)\n",
    "\n",
    "        data_points_sorted = data_points[np.argsort(data_points[:, idx_column])]\n",
    "\n",
    "        # sort the data_points in base of idx_column\n",
    "\n",
    "\n",
    "        n = len(data_points_sorted)\n",
    "\n",
    "        m = n // 2\n",
    "\n",
    "        # label is the last element of the array in row m\n",
    "        label_m = data_points_sorted[m, -1]\n",
    "        point = data_points_sorted[m, :-1]\n",
    "        data_left = data_points_sorted[:m]\n",
    "        data_right = data_points_sorted[m+1:]\n",
    "\n",
    "        node = KdNode(point=point , label=label_m, idx_column=idx_column)\n",
    "        node.left = self._build_tree(data_left, depth+1)\n",
    "        node.right = self._build_tree(data_right, depth+1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _knn_search(self, node, point, k, max_heap):\n",
    "        if node is None:\n",
    "            return\n",
    "        axis = node.idx_column\n",
    "        current_dist = distance(point, node.point)\n",
    "        if len(max_heap) < k:\n",
    "            heapq.heappush(max_heap, (-1 * current_dist, node))\n",
    "        elif current_dist < -1 * max_heap[0][0]:  #\n",
    "            # pop the node with minimum (-1*dist) and replace with (-current_dist , node)\n",
    "            heapq.heapreplace(max_heap, (-1 * current_dist, node))\n",
    "\n",
    "        if point[axis] < node.point[axis]:\n",
    "            close_node = node.left\n",
    "            away_node = node.right\n",
    "        else:\n",
    "            close_node = node.right\n",
    "            away_node = node.left\n",
    "\n",
    "        self._knn_search(close_node, point, k, max_heap)\n",
    "        if len(max_heap) < k or abs(point[axis] - node.point[axis]) < -1 * max_heap[0][0]:\n",
    "            self._knn_search(away_node, point, k, max_heap)\n",
    "\n",
    "    def knn(self, point, k):\n",
    "        max_heap = []\n",
    "\n",
    "        self._knn_search(self.root, point, k, max_heap)\n",
    "        labels = []\n",
    "        for neighbor in max_heap:\n",
    "            labels.append(neighbor[1].label)\n",
    "\n",
    "        cont = Counter(labels)\n",
    "\n",
    "        label, _ = cont.most_common(1)[0]\n",
    "\n",
    "        return label\n",
    "    def predict(self, x):\n",
    "      return [self.knn(row , 5) for row in x]\n",
    "\n",
    "    def print_root(self):\n",
    "      self.root.show()\n",
    "\n",
    "def norm_data(data):\n",
    "    min_val = np.min(data, axis=0)\n",
    "    max_val = np.max(data, axis=0)\n",
    "    return (data - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba65a0-8825-4b6d-8ef9-309b5dca1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = norm_data(X_train_split)\n",
    "X_val_norm = norm_data(X_val)\n",
    "\n",
    "data_points_train = np.hstack((X_train_norm, Y_train_split.to_numpy().reshape(-1, 1)))\n",
    "kdtree = KdTree(data_points_train)\n",
    "\n",
    "Y_pred_knn = kdtree.predict(X_val_norm)\n",
    "\n",
    "\n",
    "# - PRECISIÓN\n",
    "accuracy_val = accuracy_score(Y_val, Y_pred_knn)\n",
    "print(f\"Precisión en el conjunto de validación: {accuracy_val * 100:.2f}%\")\n",
    "\n",
    "# - CLASIFICACION\n",
    "print(classification_report(Y_val, Y_pred_knn, target_names=labels_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48383d98-1dbc-4f6f-89b3-3c60b60927c1",
   "metadata": {},
   "source": [
    "### **Modelo 2 (opcional): Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c88dc2-df28-4960-af0d-7241813698e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPTH = 10\n",
    "MIN_SAMPLE = 3\n",
    "NINF = -float(\"inf\")\n",
    "\n",
    "def entropy(y):\n",
    "    n = len(y)\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    p = counts / n\n",
    "    h = -np.sum(p * np.log2(p))\n",
    "    return h\n",
    "\n",
    "def gini(y):\n",
    "    n = len(y)\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    p = counts / n\n",
    "    gini = 1 - np.sum(p**2)\n",
    "    return gini\n",
    "\n",
    "def get_info(parent_y, left_y, right_y, method=\"e\"):\n",
    "    peso_l = len(left_y) / len(parent_y)\n",
    "    peso_r = len(right_y) / len(parent_y)\n",
    "    \n",
    "    if method == \"e\":\n",
    "        return entropy(parent_y) - (peso_l*entropy(left_y) + peso_r*entropy(right_y)) \n",
    "    else:\n",
    "        return gini(parent_y) - (peso_l*gini(left_y) + peso_r*gini(right_y))\n",
    "\n",
    "# Nodo Decision o Nodo Hoja\n",
    "class Nodo:\n",
    "    def __init__(self, index=None, left=None, right=None, info=None, umbral=None):\n",
    "        self.index = index\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info = info\n",
    "        self.umbral = umbral\n",
    "\n",
    "        self.leaf = False\n",
    "        self.valor = None\n",
    "    \n",
    "    def make_leaf(self, y):\n",
    "        y = list(y)\n",
    "        self.valor = max(y, key=y.count)\n",
    "        self.leaf = True\n",
    "    \n",
    "# Clase para almacenar la informacion de un split\n",
    "class Split:\n",
    "    def __init__(self, index=None, left=None, right=None, info=NINF, umbral=None):\n",
    "        self.index = index\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info = info\n",
    "        self.umbral = umbral\n",
    "    \n",
    "    def update(self, new_index, new_left, new_right, new_info, new_umbral):\n",
    "        if new_info > self.info:\n",
    "            self.index = new_index\n",
    "            self.left = new_left\n",
    "            self.right = new_right\n",
    "            self.info = new_info\n",
    "            self.umbral = new_umbral\n",
    "\n",
    "    def to_node(self, left, right):\n",
    "        return Nodo(index=self.index, left=left, right=right, info=self.info, umbral=self.umbral)\n",
    "    \n",
    "# Arbol de Decision\n",
    "class DT:\n",
    "    def __init__(self, X, Y, max_depth=MAX_DEPTH, min_sample=MIN_SAMPLE, method=\"g\"):\n",
    "        self._max_depth = max_depth\n",
    "        self._min_sample = min_sample\n",
    "        self._method = method\n",
    "        self.root = self.build(np.concatenate((X, Y), axis=1))\n",
    "    \n",
    "    def build(self, data, depth=0):\n",
    "        # print(f\"node at depth: {depth}\")\n",
    "        X, Y = data[:, 0:-1], data[:, -1]\n",
    "        n, k = X.shape\n",
    "\n",
    "        if depth >= self._max_depth or n < self._min_sample:\n",
    "            return self._create_leaf(Y)\n",
    "\n",
    "        best_split = self.get_best_split(data, n, k)\n",
    "\n",
    "        if best_split.info <= 0:\n",
    "            return self._create_leaf(Y)\n",
    "\n",
    "        left_tree = self.build(best_split.left, depth + 1)\n",
    "        right_tree = self.build(best_split.right, depth + 1)\n",
    "        return best_split.to_node(left_tree, right_tree)\n",
    "    \n",
    "    def get_best_split(self, data, n, k): # n: numero de filas, k: numero de columnas\n",
    "        best_split = Split()\n",
    "\n",
    "        # optimizacion tomando un subset de features para los umbrales\n",
    "        features_subset = np.random.choice(range(k), int(np.sqrt(k)*1.5), replace=False)\n",
    "\n",
    "        for char_index in features_subset:\n",
    "            unique_values = np.unique(data[:, char_index])\n",
    "\n",
    "            for i in range(1, len(unique_values)):\n",
    "                umbral = (unique_values[i - 1] + unique_values[i]) / 2\n",
    "                data_left = data[data[:, char_index] <= umbral]\n",
    "                data_right = data[data[:, char_index] > umbral]\n",
    "\n",
    "                if len(data_left) == 0 or len(data_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                parent_y, left_y, right_y = data[:, -1], data_left[:, -1], data_right[:, -1]\n",
    "                info = get_info(parent_y, left_y, right_y, method=self._method)\n",
    "\n",
    "                if info > best_split.info:\n",
    "                    best_split.index = char_index\n",
    "                    best_split.left = data_left\n",
    "                    best_split.right = data_right\n",
    "                    best_split.info = info\n",
    "                    best_split.umbral = umbral\n",
    "\n",
    "        return best_split\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict_recursive(self.root, fila) for fila in X]\n",
    "        return np.array(y_pred) \n",
    "\n",
    "    def _predict_recursive(self, nodo, x):\n",
    "        if nodo.leaf:\n",
    "            return nodo.valor\n",
    "\n",
    "        # decidir el hijo por el cual seguir en el nodo decision\n",
    "        if x[nodo.index] <= nodo.umbral:\n",
    "            return self._predict_recursive(nodo.left, x)\n",
    "        else:\n",
    "            return self._predict_recursive(nodo.right, x)\n",
    "\n",
    "    def _create_leaf(self, Y):\n",
    "        leaf = Nodo()\n",
    "        leaf.make_leaf(Y)\n",
    "        return leaf\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Ejemplo de uso:\n",
    "    X = x_train\n",
    "    Y = y_train.reshape(-1, 1)\n",
    "    model = DT(X, Y) // DT(X, Y, max_depth=20, min_sample=5, method=\"e\")\n",
    "    y_pred = model.predict(x_test)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c0a8c8-fcc1-479c-87cb-bf196cc9b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DT(X_train_split, Y_train_split.to_numpy().reshape(-1, 1), max_depth=15, method='g')\n",
    "Y_pred_tree = dt.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cceb9a6-1d86-4223-a82f-dbc78c49863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - PRECISIÓN\n",
    "accuracy_val_tree = accuracy_score(Y_val, Y_pred_tree)\n",
    "print(f\"Precisión en el conjunto de validación: {accuracy_val_tree * 100:.2f}%\")\n",
    "\n",
    "# - CLASIFICACIÓN\n",
    "print(classification_report(Y_val, Y_pred_tree, target_names=labels_actions))\n",
    "\n",
    "# - MATRIZ DE CONFUSIÓN\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix(Y_val, Y_pred_tree), annot=True, fmt='g', cbar=False)\n",
    "plt.xlabel('Prediccion')\n",
    "plt.ylabel('Valor real')\n",
    "plt.title('Matriz de Confusion - DT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2388651",
   "metadata": {},
   "source": [
    "---\n",
    "### **Modelo 03: Regresión logística multivariada**\n",
    "\n",
    "Para poder implementar correctamente la regresión logística que se tenía (desarrollada en laboratorios) se tuvo que modificar ciertos aspectos, dado que ahora contamos con labels del 1 al 6, es decir ya no podemos usar una clasificación binaria, si no una que involucre multivariables.\n",
    "\n",
    "El primer cambio realizado fue utilizar una **función softmax** definida como `softmax`, en lugar de una sigmoide `s`. Esto se debe a que según nuestra [investigación](https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier) notamos que la función sigmoide está restringida a una clasificación logística binaria: 2 clases, mientras que la softmax permite una clasificación de múltiples clases. Matemáticamente, este método sólo extiende la fórmula de regresión logística para $k$ clases.\n",
    "\n",
    "Para el caso de la función pérdida, apartado donde se mide el error de la predicción de clases se actualizó de binary cross entropy a cross entropy con una regularización L2 (ridge) para manejar múltiples clases, la regularización elegida tiene como objetivo penalizar grandes valores y así evitar el overfitting. Se ejecuta softmax a la hora de predecir y busca entre todas las clases de predicción la que tenga mayor probabilidad.\n",
    "\n",
    "Por otro lado, para poder utilizar las etiquetas que se indicaron en el problema, primero obtenemos los valores únicos para el eje Y, una vez los tenemos, debemos de usar un formato para entrenar las multiclases, por lo que es necesario contar con una matriz de tamaño $n x k$, siendo $n$ la cantidad de filas de $Y$, mientras que $k$ la cantidad de clases. Se traza una diagonal de $1$ para la correspondiente categoría indicando su importancia. Es decir se transforman las variables en vectores binarios $(1, 0)$ para usarse en la función pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70430028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    y = y.astype(int) - 1;\n",
    "    \n",
    "    encoded_matrix = np.zeros((y.shape[0], num_classes))\n",
    "    encoded_matrix[np.arange(y.shape[0]), y] = 1\n",
    "    return encoded_matrix\n",
    "\n",
    "class MultivariateLogisticRegression():\n",
    "    def __init__(self, alpha=0.01, epochs=10000, epsilon=0.00001, reg_ridge=0.001):\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.epsilon = epsilon\n",
    "        self.reg_ridge = reg_ridge\n",
    "        self.weights = None\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def loss(self, y, y_approx):\n",
    "        n = len(y)\n",
    "        y_approx = np.clip(y_approx, self.epsilon, 1 - self.epsilon)\n",
    "        L = -np.sum(y * np.log(y_approx)) / n\n",
    "        L += self.reg_ridge * np.sum(self.weights**2) / (2 * n)\n",
    "        \n",
    "        return L\n",
    "\n",
    "    def derivatives(self, x, y):\n",
    "        y_approx = self.softmax(np.dot(x, self.weights))\n",
    "        dw = np.dot(x.T, (y_approx - y)) / len(y)\n",
    "        dw += self.reg_ridge * self.weights / len(y)\n",
    "        \n",
    "        return dw\n",
    "\n",
    "    def update_parameters(self, derivatives):\n",
    "        self.weights -= self.alpha * derivatives\n",
    "\n",
    "    def train(self, x, y):\n",
    "        np.random.seed(11)\n",
    "        self.weights = np.random.rand(x.shape[1], y.shape[1])\n",
    "        loss_vec = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            y_approx = self.softmax(np.dot(x, self.weights))\n",
    "            loss_value = self.loss(y, y_approx)\n",
    "            dw = self.derivatives(x, y)\n",
    "            self.update_parameters(dw)\n",
    "            loss_vec.append(loss_value)\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f'Número de epoch #{epoch}, Valor de pérdida: {loss_value}')\n",
    "\n",
    "        return loss_vec\n",
    "\n",
    "    def predict(self, x):\n",
    "        probabilities = self.softmax(np.dot(x, self.weights))\n",
    "        \n",
    "        return np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1440c01",
   "metadata": {},
   "source": [
    "Se procede a hacer la normalización de datos por medio de `StandardScaler()`. Además se preparan los datos, es decir por medio de la función softmax que recibe un vector $[1, 0, 0, \\dots , 0]$, y lo convierte a un vector de probabilidades donde $s = [s_1, s_2, \\dots, s_k]$ es la salida, tal que la suma de todas las clases es $1$, indicando la probabilidad de que una muestra pertenezca a cada una de las clases posibles, recordando que por el enconding las columnas indican las clases denotadas como $k$.\n",
    "\n",
    "$$\n",
    "softmax(s_i) = \\frac{e^{s_i}}{\\sum_j^c e^{s_j}}\n",
    "$$\n",
    "\n",
    "Es así que con softmax logramos tener un vector de probabilidades que nos va servir para interpretar la probabilidad de que una muestra pertenezca a una clase específica. Su implementación en [python](https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python) se realiza con el cálculo de exponenciales dividido entre su suma para convertir en decimales que indiquen probabilidad, osea suma de $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23a05de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de epoch #0, Valor de pérdida: 3.4902828660775613\n",
      "Número de epoch #1000, Valor de pérdida: 0.19666453743367185\n",
      "Número de epoch #2000, Valor de pérdida: 0.181063566547272\n",
      "Número de epoch #3000, Valor de pérdida: 0.17346230252345302\n",
      "Número de epoch #4000, Valor de pérdida: 0.16850005249140168\n",
      "Número de epoch #5000, Valor de pérdida: 0.16481179670561577\n",
      "Número de epoch #6000, Valor de pérdida: 0.1618681643602175\n",
      "Número de epoch #7000, Valor de pérdida: 0.15941863834230285\n",
      "Número de epoch #8000, Valor de pérdida: 0.1573332042965109\n",
      "Número de epoch #9000, Valor de pérdida: 0.15553423104162492\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           WALKING       0.93      0.95      0.94       247\n",
      "  WALKING_UPSTAIRS       0.93      0.95      0.94       200\n",
      "WALKING_DOWNSTAIRS       0.98      0.94      0.96       206\n",
      "           SITTING       0.89      0.89      0.89       262\n",
      "          STANDING       0.90      0.90      0.90       276\n",
      "            LAYING       1.00      1.00      1.00       280\n",
      "\n",
      "          accuracy                           0.94      1471\n",
      "         macro avg       0.94      0.94      0.94      1471\n",
      "      weighted avg       0.94      0.94      0.94      1471\n",
      "\n",
      "Exactitud del modelo en validación: 93.81%\n"
     ]
    }
   ],
   "source": [
    "# tecnica de división de datos en entrenamiento y validación\n",
    "labels_actions = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING']\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_split_scaled = scaler.fit_transform(X_train_split)\n",
    "X_val_split_scaled = scaler.transform(X_val_split)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "num_classes = len(np.unique(y_train_split))\n",
    "Y_train_encoded = one_hot_encode(y_train_split.to_numpy().flatten().astype(int), num_classes)\n",
    "\n",
    "model = MultivariateLogisticRegression(alpha=0.2, epochs=10000, reg_ridge=0.1)\n",
    "loss_vec = model.train(X_train_split_scaled, Y_train_encoded)\n",
    "\n",
    "# se suma 1 a la prediccion por notacion de kaggle\n",
    "y_pred = model.predict(X_val_split_scaled) + 1\n",
    "accuracy = accuracy_score(y_val_split, y_pred)\n",
    "\n",
    "y_val_pred = model.predict(X_val_split_scaled) + 1\n",
    "accuracy = accuracy_score(y_val_split, y_val_pred)\n",
    "\n",
    "print(classification_report(y_val_split, y_val_pred, target_names=labels_actions))\n",
    "print(f'Exactitud del modelo en validación: {accuracy * 100:.2f}%')\n",
    "\n",
    "y_test_pred = model.predict(X_test_scaled) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa36506b-cbae-4f98-9e20-3eb7e0703aeb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4313e5-72ac-49bf-9985-41a3b05d97f6",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d35d91-3c9c-4022-9278-ec1805a01502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91356aff-b90d-4cc0-9d3c-810b7ae5d1f8",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb09890-ea96-478f-8d30-70c64051e3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5dd7ec1-40c8-4819-aa67-f77b1b15ab1a",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54841f6a-aa2a-45dc-89f0-e86c858243f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm = norm_data(X_test.to_numpy())\n",
    "\n",
    "# Y_test_knn = kdtree.predict(X_test_norm)\n",
    "Y_test_knn = np.array(Y_test_knn)\n",
    "print(Y_test_knn.shape)\n",
    "print(np.unique(Y_test_knn))\n",
    "\n",
    "# Y_test_tree = dt.predict(X_test.to_numpy())\n",
    "# print(Y_test_tree.shape)\n",
    "# print(np.unique(Y_test_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbdad9-d896-45fb-9c12-3787854e8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribiendo a csv\n",
    "output = \"results.csv\"\n",
    "\n",
    "result_csv = pd.DataFrame(Y_test_knn.astype(int))\n",
    "# result_csv = pd.DataFrame(Y_test_tree.astype(int))\n",
    "result_csv.head()\n",
    "result_csv = result_csv.reset_index()\n",
    "result_csv['index'] += 1\n",
    "\n",
    "result_csv.rename(columns={'index': 'ID', 0: 'y'}, inplace=True)\n",
    "result_csv.to_csv(output, index=False)\n",
    "print(f\"Wrote to: {output} !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
